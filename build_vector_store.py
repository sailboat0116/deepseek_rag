import os
import faiss
import pickle
from docx import Document
from sentence_transformers import SentenceTransformer

def read_docx(path):
    doc = Document(path)
    texts = []

    # æ­£æ–‡æ®µè½
    for p in doc.paragraphs:
        if p.text.strip():
            texts.append(p.text.strip())

    # è¡¨æ ¼å…§å®¹ï¼ˆå¯é¸ï¼‰
    for table in doc.tables:
        for row in table.rows:
            for cell in row.cells:
                if cell.text.strip():
                    texts.append(cell.text.strip())

    print(f"ğŸ“„ è®€å– {os.path.basename(path)}ï¼Œç¸½æ®µè½æ•¸ï¼š{len(texts)}")
    return "\n".join(texts)

def chunk_text(text, max_length=500):
    sentences = text.replace("\n", " ").split(". ")
    chunks, current = [], ""
    for s in sentences:
        if len(current) + len(s) < max_length:
            current += s + ". "
        else:
            chunks.append(current.strip())
            current = s + ". "
    if current:
        chunks.append(current.strip())
    return chunks

def build_faiss_index(chunks, model_name="all-MiniLM-L6-v2"):
    model = SentenceTransformer(model_name)
    embeddings = model.encode(chunks)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    return index, chunks

if __name__ == "__main__":
    folder = r"C:\Users\sailboat\data"
    all_chunks = []

    for filename in os.listdir(folder):
        if filename.lower().endswith(".docx"):
            path = os.path.join(folder, filename)
            try:
                text = read_docx(path)
                chunks = chunk_text(text)
                all_chunks.extend(chunks)
            except Exception as e:
                print(f"âŒ ç„¡æ³•è®€å– {filename}ï¼š{e}")

    print(f"\nğŸ§© å…¨éƒ¨æ–‡ä»¶åˆä½µå¾Œï¼Œç¸½å…±åˆ†æ®µï¼š{len(all_chunks)}")

    if not all_chunks:
        print("âŒ æ²’æœ‰æœ‰æ•ˆæ®µè½ï¼Œè«‹æª¢æŸ¥ Word æ–‡ä»¶æ˜¯å¦æœ‰å…§å®¹ã€‚")
        exit()

    index, docs = build_faiss_index(all_chunks)
    faiss.write_index(index, "vector.index")
    with open("docs.pkl", "wb") as f:
        pickle.dump(docs, f)

    print("âœ… å‘é‡è³‡æ–™åº«å·²å»ºç«‹å®Œæˆï¼")
